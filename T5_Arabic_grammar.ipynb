{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9489774,"sourceType":"datasetVersion","datasetId":5773594},{"sourceId":104449,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-08T06:35:11.920282Z","iopub.execute_input":"2024-10-08T06:35:11.920677Z","iopub.status.idle":"2024-10-08T06:35:13.038393Z","shell.execute_reply.started":"2024-10-08T06:35:11.920626Z","shell.execute_reply":"2024-10-08T06:35:13.037339Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n/kaggle/input/dataset/quran_texts.txt\n/kaggle/input/dataset/scraping.py\n/kaggle/input/dataset/README.md\n/kaggle/input/dataset/Quran_Grammar.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, Trainer, TrainingArguments\nfrom transformers import PreTrainedTokenizerFast,AutoTokenizer,AutoModelForCausalLM\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import LlamaForCausalLM, Trainer, TrainingArguments\nfrom IPython.display import display, Markdown\nfrom datasets import Dataset\nimport re\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:35:13.040201Z","iopub.execute_input":"2024-10-08T06:35:13.040675Z","iopub.status.idle":"2024-10-08T06:35:33.692257Z","shell.execute_reply.started":"2024-10-08T06:35:13.040627Z","shell.execute_reply":"2024-10-08T06:35:33.691279Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load your training dataset (replace 'my_dataset' with your actual data\ndata=pd.read_csv(\"/kaggle/input/dataset/Quran_Grammar.csv\",sep=';')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:35:33.693366Z","iopub.execute_input":"2024-10-08T06:35:33.693973Z","iopub.status.idle":"2024-10-08T06:35:33.764599Z","shell.execute_reply.started":"2024-10-08T06:35:33.693938Z","shell.execute_reply":"2024-10-08T06:35:33.763790Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def clean_grammar_text(text):\n    # Remove any content inside brackets including the brackets themselves\n    cleaned_text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\{.*?\\}', '', text)\n    # Remove specific characters \"﴾\" and \"﴿\"\n    cleaned_text = cleaned_text.replace(\"﴾\", \"\").replace(\"﴿\", \"\")\n    # Remove extra spaces\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:35:33.766864Z","iopub.execute_input":"2024-10-08T06:35:33.767172Z","iopub.status.idle":"2024-10-08T06:35:33.772726Z","shell.execute_reply.started":"2024-10-08T06:35:33.767141Z","shell.execute_reply":"2024-10-08T06:35:33.771673Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data['grammar'] = data['grammar'].apply(clean_grammar_text)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:35:33.773866Z","iopub.execute_input":"2024-10-08T06:35:33.774165Z","iopub.status.idle":"2024-10-08T06:35:33.902607Z","shell.execute_reply.started":"2024-10-08T06:35:33.774134Z","shell.execute_reply":"2024-10-08T06:35:33.901860Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"ahmeddbahaa/t5-arabic-base-finetuned-wikilingua-ar\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ahmeddbahaa/t5-arabic-base-finetuned-wikilingua-ar\")","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:35:33.903780Z","iopub.execute_input":"2024-10-08T06:35:33.904063Z","iopub.status.idle":"2024-10-08T06:35:55.460782Z","shell.execute_reply.started":"2024-10-08T06:35:33.904032Z","shell.execute_reply":"2024-10-08T06:35:55.459848Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/415 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93b6aa7c55c744bab99ce3135ec1dc7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/847k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a84f4ab0332431d943f0d759dfe7580"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd1dd20215dc49b7bb5ab2cebc9e5573"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16d4971076ac41618b1f1bd75f3ad08f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/785 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c58722f8dfc4596a76d7064127a55bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2e16f057b94cb7addda24d0e33c0ad"}},"metadata":{}}]},{"cell_type":"code","source":"class ArabicGrammarDataset(Dataset):\n    def __init__(self, verses, grammars, tokenizer, max_length=512):\n        self.verses = verses\n        self.grammars = grammars\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.verses)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, list):\n        # Handle the batch case\n            input_texts = [self.verses[i] for i in idx]\n            target_texts = [self.grammars[i] for i in idx]\n        else:\n        # Handle the single case\n            input_texts = [self.verses[idx]]\n            target_texts = [self.grammars[idx]]\n\n    # Tokenize inputs and targets\n        inputs = self.tokenizer(input_texts, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n        targets = self.tokenizer(target_texts, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n\n    # Return tokenized inputs and targets without flattening\n        return {\n            'input_ids': inputs['input_ids'],  # Keep as 2D tensor\n            'attention_mask': inputs['attention_mask'],  # Keep as 2D tensor\n            'labels': targets['input_ids'],  # Keep as 2D tensor\n        }\n\n# Create dataset\ndataset = ArabicGrammarDataset(data['verse'].tolist(), data['grammar'].tolist(), tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:35:55.462333Z","iopub.execute_input":"2024-10-08T06:35:55.462736Z","iopub.status.idle":"2024-10-08T06:35:55.473867Z","shell.execute_reply.started":"2024-10-08T06:35:55.462694Z","shell.execute_reply":"2024-10-08T06:35:55.472786Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom transformers import get_scheduler\n\n# Define training parameters\nnum_epochs = 5\nbatch_size = 3\nlearning_rate = 5e-5\n\n# Create DataLoader\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize optimizer\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\n# Define scheduler\nnum_training_steps = num_epochs * len(train_loader)\nscheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:35:55.475239Z","iopub.execute_input":"2024-10-08T06:35:55.475552Z","iopub.status.idle":"2024-10-08T06:35:57.509062Z","shell.execute_reply.started":"2024-10-08T06:35:55.475512Z","shell.execute_reply":"2024-10-08T06:35:57.507931Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(35000, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(35000, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(35000, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=35000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.train()\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        # Move batch to device\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        # Forward pass\n        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n        loss = outputs.loss\n\n        # Backward pass\n        loss.backward()\n\n        # Update parameters\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {loss.item():.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:35:57.511531Z","iopub.execute_input":"2024-10-08T06:35:57.511879Z","iopub.status.idle":"2024-10-08T06:51:24.796479Z","shell.execute_reply.started":"2024-10-08T06:35:57.511838Z","shell.execute_reply":"2024-10-08T06:51:24.795471Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/5 - Loss: 1.8032\nEpoch 2/5 - Loss: 1.7071\nEpoch 3/5 - Loss: 2.3307\nEpoch 4/5 - Loss: 1.0438\nEpoch 5/5 - Loss: 0.7721\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_grammar(verse):\n    model.eval()  # Set the model to evaluation mode\n    input_ids = tokenizer(verse, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(device)\n    attention_mask = tokenizer(verse, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).attention_mask.to(device)\n\n    # Generate the output\n    with torch.no_grad():\n        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n    \n    # Decode the output\n    grammar = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return grammar\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:53:57.942910Z","iopub.execute_input":"2024-10-08T06:53:57.943314Z","iopub.status.idle":"2024-10-08T06:53:57.950034Z","shell.execute_reply.started":"2024-10-08T06:53:57.943278Z","shell.execute_reply":"2024-10-08T06:53:57.949008Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Sample verses to test\ntest_verses = [\n    \"بِسْمِ اللهِ الرَّحْمنِ الرَّحِيمِ\",\n    \"الْحَمْدُ لِلّهِ رَبِّ الْعالَمِينَ\"\n]\n\n# Generate grammar for each verse\nfor verse in test_verses:\n    generated_grammar = generate_grammar(verse)\n    print(f\"Verse: {verse}\\nGenerated Grammar: {generated_grammar}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:53:58.955906Z","iopub.execute_input":"2024-10-08T06:53:58.956293Z","iopub.status.idle":"2024-10-08T06:53:59.712580Z","shell.execute_reply.started":"2024-10-08T06:53:58.956254Z","shell.execute_reply":"2024-10-08T06:53:59.711682Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Verse: بِسْمِ اللهِ الرَّحْمنِ الرَّحِيمِ\nGenerated Grammar: : فعل ماض و: فاعل. : فعل ماض و:\n\nVerse: الْحَمْدُ لِلّهِ رَبِّ الْعالَمِينَ\nGenerated Grammar: وَلّهِ: جار ومجرور متعلق ب. ر\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}